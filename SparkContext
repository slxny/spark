from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName('my-app').setMaster(master)
sc = SparkContext(conf=conf)

# sc is just the naming convention for SparkContext
# use setMaster with caution (can create security incident)
# usually it is recommended not to use this particular methor passing master to setMaster as it may include credentials


sparkConf()
set(key, value) cores, memory, etc.

setMaster(MASTER_URL) - configure Spark master url

setAppName - configure app name that gets displayed in Spark UI

setSparkHome(PATH) configure path to Spark on the worker nodes

A transformatioin in a nutshell takes an RDD or DataFrame and produces a new RDD or DataFrame as output:

Common Transforms: 
  - map
  - filter
  - flatMap
  - reduceByKey
  - groupByKey
  - sortBy
